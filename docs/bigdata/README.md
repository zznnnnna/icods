## 聊聊大数据

#### <font color=#090909>什么是大数据？</font>

 我们看一下百度百科的说法：

-  大数据(big data)，或称巨量资料，指的是所涉及的资料量规模巨大到无法透过主流软件工具，在合理时间内达到撷取、管理、处理、并整理成为帮助企业经营决策更积极目的的资讯。 [19] 

-  在[维克托·迈尔-舍恩伯格](https://baike.baidu.com/item/维克托·迈尔-舍恩伯格)及肯尼斯·库克耶编写的《[大数据时代](https://baike.baidu.com/item/大数据时代/15434499)》 [1] 中大数据指不用随机分析法（[抽样调查](https://baike.baidu.com/item/抽样调查)）这样捷径，而采用所有数据进行分析处理。大数据的5V特点（IBM提出）：[Volume](https://baike.baidu.com/item/Volume/17610592)（大量）、[Velocity](https://baike.baidu.com/item/Velocity/1398152)（高速）、[Variety](https://baike.baidu.com/item/Variety/191328)（多样）、[Value](https://baike.baidu.com/item/Value/2285610)（低价值密度）、[Veracity](https://baike.baidu.com/item/Veracity/19362178)（真实性）。

-  可以看出大数据的特点主要是数据量大，而处理这些数据量大的数据就需要用到大数据开发技术，在此之上不断地优化性能与执行速度。而对于如今的互联网来说，最重要的无非就是数据。而这样的发展趋势就造就了一个新的职业，大数据开发工程师，因此学习大数据开发是一个不错的选择。

#### <font color=#090909>Java转大数据合适吗？</font>

从目前大数据的发展和趋势来看，越来越多的开发人员开始转向大数据转型。主要原因我认为有以下几点。

-  基于目前互联网的大趋势，市场上对于大数据开发程序员的需求量还是很大的，这一主要原因无非就是，互联网产品架构逐渐成熟，而在此大量的数据开始涌入，不得不说是一个很好的机会。
- 其实Java程序员转型大数据有得天独厚的优势，大数据技术很多技术框架的底层都跟Java息息相关，提供了完善的Java调用接口。其次大数据中另外一门重要的语言Scala也是基于Java的，因此在技术难度上Java开发的转型是没有问题的。
- 最重要的一点，在同等开发经验中，大数据开发在薪资上是压Java开发一头的。并且，大数据开发对经验需求很高，可谓越老越吃香，对于以后的发展也是很有益的。

-  综上所述，我个人认为Java转大数据很合适。

#### <font color=#090909>Java转型大数据开发都需要掌握哪些技术？</font>

针对目前大数据的情况来说，学习大数据需要的技术可以从开发语言、开发框架方面说起。

> ##### <font color=#000000> Linux</font>

毋庸置 疑Linux对于学习大数据是很重要的，大数据开发的基础环境都在Linux操作系统上部署，所以我们应该掌握常用的Linux命令以及熟悉Shell编程。

> ##### <font color=#000000>Java</font>

如果说学习大数据最重要的开发语言一定是Java，首先大数据技术源于最初的Hadoop生态圈，而Hadoop生态圈中的技术大都是Java编写，要么是与Java密切相关的Scala

> ##### <font color=#000000>Scala</font>

对于大数据开发另外一门重要的开发语言是Scala，就如刚才所说Spark、Flink底层都有用到Scala，而Scala语言是一门纯粹的面向对象编程语言，而又无缝地结合了命令式编程和[函数式编程](https://baike.baidu.com/item/函数式编程)风格。在大数据开发中占据了非常重要的位置。虽然基于Scala开发的技术也有有相关的Java的调用方式，但是在代码量的方面来说Scala是完成Java的，Scala中的函数式编程启动来很重要的作用。

> ##### <font color=#000000>Python</font>

对于大数据学习到一定的深度的时候，需要涉及深度学习、大数据分析等，此时Python就占据了很重要的位置。在机器学习领域，python是很重要的一门开发语言。

> ##### <font color=#000000>Hadoop</font>

Hadoop得以在大数据处理应用中广泛应用得益于其自身在数据提取、变形和加载(ETL)方面上的天然优势。Hadoop的分布式架构，将大数据处理引擎尽可能的靠近存储，对例如像ETL这样的批处理操作相对合适，因为类似这样操作的批处理结果可以直接走向存储。Hadoop的MapReduce功能实现了将单个任务打碎，并将碎片任务(Map)发送到多个节点上，之后再以单个数据集的形式加载(Reduce)到数据仓库里 。

> ##### <font color=#000000>HDFS</font>

HDFS全称是Hadoop Distributed File System，Hadoop的分布式文件系统，是一种允许文件通过网络在多台主机上分享的文件系统，可以让多台机器上的多个用户分享文件和存储空间。其中HDFS相对于其他的分布式文件存储系统不同的特点是。HDFS是一种适合大文件存储的分布式文件系统，不适合小文件存储。

> ##### <font color=#000000>MapReduce</font>

相对于HDFS来说，如果只是把文件数据存储起来，意义并不大。而MapReduce是一个用来快速计算这些海量数据的框架。MapReduce是分布式运行的，由两个阶段组成：Map和Reduce。Map阶段是一个独立的程序，在很多节点同时运行每个节点同时处理一部分数据，Reduce阶段也是一个独立的程序。可以在一个或者多个节点同时运行，每个节点处理一部分数据。Map是对数据进行局部汇总，Reduce就是对局部数据进行最终汇总。

>##### <font color=#000000>Hive</font>

Hive是建立在Hadoop上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载，可以简称为ETL。Hive 定义了简单的类SQL查询语言，称为HQL，它允许熟悉SQL的用户直接查询Hadoop中的数据，同时，这个语言也允许熟悉MapReduce的开发者开发自定义的mapreduce任务来处理内建的SQL函数无法完成的复杂的分析任务。Hive中包含的有SQL解析引擎，它会将SQL语句转译成M/R Job,然后在Hadoop中执行。通过这里的分析我们可以了解到Hive可以通过sql查询Hadoop中的数据，并且sql底层也会转化mapreduce任务，所以hive是基于hadoop的。

> ##### <font color=#000000>Flume</font>

Flume是一个高可用，高可靠，分布式的海量日志采集、聚合和传输的系统，能够有效的收集、聚合、移动大量的日志数据。其实通俗一点来说就是Flume是一个很靠谱，很方便、很强的日志采集工具。它是目前大数据领域数据采集最常用的一个框架。

> ##### <font color=#000000>Spark</font>

Spark是一个用于大规模数据处理的统一计算引擎，注意：Spark不仅仅可以做类似于MapReduce的离线数据计算，还可以做实时数据计算，并且它还可以实现类似于Hive的SQL计算，等等，所以说它是一个统一的计算引擎。既然说到了Spark，那就不得不提一下Spark里面最重要的一个特性：内存计算，Spark中一个最重要的特性就是基于内存进行计算，从而让它的计算速度可以达到MapReduce的几十倍甚至上百倍。Spark提供了Core、SQL、Streaming、MLlib、GraphX等技术组件，可以一站式地完成大数据领域的离线批处理、SQL交互式查询、流式实时计算，机器学习、图计算等常见的任务从这可以看出来Spark也是一个具备完整生态圈的技术框架。

> ##### <font color=#000000>Kafka</font>

Kafka是消息队列技术中唯一的大数据专用消息队列技术，主要用于大数据的日志采集，什么是消息队列？消息队列（Message Queue）简称MQ，是针对于如今请求数据量过大的情况下，诞生的一种中间件。顾名思义就是 消息+队列，其实就是保存消息的队列，属于消息传输过程中的容器。消息队列主要提供生产、消费接口供外部调用，做数据的存储和读取。其他常用的消息队列有RocketMq、RabbitMQ。

>##### <font color=#000000>Redis</font>

Redis是一种面向 “Key-Value” 数据类型的内存数据库，可以满足我们对海量数据的快速读写需求,注意：首先Redis是一种内存数据库，它的数据都是放在内存里面的，,然后Redis中存储的数据都是key-value类型的,其中redis中的key只能是字符串，value支持多种数据类型.

> ##### <font color=#000000>Flink</font>

Apache Flink 是一个开源的分布式，高性能，高可用，准确的流处理框架。分布式：表示Flink程序可以运行在很多台机器上，高性能：表示Flink处理性能比较高，高可用：表示Flink支持程序的自动重启机制。准确的：表示Flink可以保证处理数据的准确性。Flink支持流处理和批处理，虽然我们常说Flink是一个流处理框架，但是它也支持批处理。其实对于Flink而言，它是一个流处理框架，批处理只是流处理的一个极限特例而已。

#### <font color=#090909>网站内容有什么？</font>

-  本站大数据开发模块，会用来分享我作为一个Java开发转型大数据的学习笔记以及经验分享。希望能够帮到更多与我经历相同的小伙伴。笔记的方向大致跟上述的技术栈有关。

#### <font color=#090909>公众号[大数据笔记本]</font>

-  认为文章写的不错的或者对于大数据开发有不一样的见解和分析的小伙伴，欢迎关注我的公众号[大数据笔记本]及时获取最新动态。

<img src="../style/bigdataqr.png" alt="大数据笔记本" style="zoom:50%;" />



